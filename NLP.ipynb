import csv
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
from sklearn import metrics
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB

from keras import backend as K
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dense,Dropout,Flatten
from keras.layers import Activation, Conv1D, GlobalMaxPooling1D
from keras import optimizers

from keras.utils import to_categorical

URL_Tr = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'
URL_Te = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/test.tsv'

X_train = pd.read_csv(URL_Tr,sep='\t')
X_test = pd.read_csv(URL_Te,sep='\t')

#train.head()


fullSent = X_train.loc[X_train.groupby('SentenceId')['PhraseId'].idxmin()]

fullSent.head()

#imp line
BoW_vectorizer = CountVectorizer(strip_accents='unicode',
                                 stop_words=None,#["\'",",",".","a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "all", "almost", "alone", "along", "already", "also","although","always","am","among", "amongst", "amoungst", "amount",  "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "around", "as",  "at", "back","be","became", "because","become","becomes", "becoming", "been", "before", "beforehand", "behind", "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry", "de", "describe", "detail", "do", "done", "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", "front", "full", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "hundred", "ie", "if", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter", "latterly", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own","part", "per", "perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", "she", "should", "show", "side", "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "system", "take", "ten", "than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they", "thick", "thin", "third", "this", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "top", "toward", "towards", "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "we", "well", "were", "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", "why", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves", "the"],
                                 ngram_range=(1,4),
                                 analyzer='word',
                                 min_df=5,
                                 max_df=.5)

BoW_vectorizer.fit(list(fullSent['Phrase']))

phrase = X_train['Phrase']
sentiment = X_train['Sentiment']

X_train, X_test,Y_train,Y_test=train_test_split(phrase,sentiment,test_size=0.2,random_state=4)

Y_train = to_categorical(Y_train)
#X_test = to_categorical(X_test)

Y_train.shape
#print(X_train.shape,"\n")

train_bow = BoW_vectorizer.transform(X_train)
test_bow = BoW_vectorizer.transform(X_test)

bow_feature_vec = pd.DataFrame(test_bow.toarray(), columns=BoW_vectorizer.get_feature_names())
#bow_feature_vec.head(15)

def recall_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
  recall = true_positives / (possible_positives + K.epsilon())
  return recall

def precision_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
  precision = true_positives / (predicted_positives + K.epsilon())
  return precision

def f1_m(y_true, y_pred):
  precision = precision_m(y_true,y_pred)
  recall = recall_m(y_true, y_pred)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

def baseline_cnn_model(fea_matrix, n_class, mode, compiler):
  model = Sequential()
  model.add(Conv1D(filters=64, kernel_size=1, activation='relu',
                  input_shape=(fea_matrix.shape[1],fea_matrix.shape[2])))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Activation('relu'))
  model.add(Dense(n_class))
  if n_class == 1 and mode == "cla":
    model.add(Activation('sigmoid'))
    model.compile(optimizer=compiler,loss='binary_crossentropy',
                 metrics=['acc',f1_m,precision_m,recall_m])
  else:
    model.add(Activation('softmax'))
    model.compile(optimizer=compiler,loss='categorical_crossentropy',
                 metrics=['acc',f1_m,precision_m,recall_m])
  return model

try:
  train_bow = np.array(train_bow.toarray())
  train_bow = train_bow.reshape(train_bow.shape[0], train_bow.shape[1], 1)
  test_bow = np.array(test_bow.toarray())
  test_bow = test_bow.reshape(test_bow.shape[0], test_bow.shape[1], 1)
except:
  pass

train_bow.shape

lr = 1e-3
batch_size = 128
num_epochs =100
decay=1e-4
mode="reg"
n_class = 5

adm = optimizers.Adam(lr=lr,decay=decay)
sgd = optimizers.SGD(lr=lr,nesterov=True,momentum=0.7,decay=decay)
Nadam = optimizers.Nadam(lr=lr,beta_1=0.9,beta_2=0.999, epsilon=1e-08)
model = baseline_cnn_model(train_bow,n_class,mode,adm)

y_train_binary = to_categorical(Y_train)
y_test_binary = to_categorical(Y_test)

#print(y_train_binary)

for epoch in range(num_epochs):
  print(epoch+1,'/',num_epochs)
  model1=model.fit(train_bow,Y_train,batch_size=batch_size,
         epochs=1,verbose=1,validation_split=0.2)
  #print(model.evaluate(test_bow, y_test_binary))



def print_metrics(accuracy, f1_score,precision,recall):
  print('\n')
  print('Simple CNN model Performance')
  print('Accuracy  : ', np.round(accuracy, 4))
  print('Precision : ',np.round(precision, 4))
  print('Recall    : ',np.round(recall, 4))
  print('F1 Score  : ',np.round(f1_score, 4))
  print('\n')
  
loss, accuracy, f1_score, precision, recall = model.evaluate(test_bow,y_test_binary)
print_metrics(accuracy, f1_score,precision,recall)